import os
import re
import json
import pandas as pd
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
from difflib import get_close_matches
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List, Dict
from openai import OpenAI
from crewai.tools import BaseTool

# Load API key
load_dotenv("C:/Users/hp/Documents/Agent Router Tools/.env")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Output directory
OUTPUT_DIR = Path("regulatory_outputs/site_outputs")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Input model
class LLMExtractorInput(BaseModel):
    url: str = Field(..., description="Original URL")
    extracted_file: str = Field(..., description="Path to .txt file generated by HTML Extractor")

# Tool
class LLMExtractorTool(BaseTool):
    name: str = "llm_extractor_tool"
    description: str = "Extracts structured regulatory updates from extracted text and inferred links"
    args_schema: type = LLMExtractorInput

    def _run(self, url: str, extracted_file: str) -> Dict:
        if not os.path.exists(extracted_file):
            raise FileNotFoundError(f"❌ Extracted .txt file not found: {extracted_file}")

        with open(extracted_file, "r", encoding="utf-8") as f:
            extracted_text = f.read()

        # Extract links using regex
        extracted_links = re.findall(r'\((https?://[^\s)]+)\)', extracted_text)

        # Construct prompt for LLM
        prompt = f"""
You are a regulatory update extraction assistant.

From the following DOCUMENT CONTENT, extract each distinct regulatory update.
Return the output as a strict JSON array of objects, each with the following keys:
- "date": the date of the update in YYYY-MM-DD format (if available)
- "topic": short title or subject of the update
- "additional_context": supporting detail or summary text
- "link": full URL to the source (choose from known_links)
- "regulator": the issuing regulatory body

⚠️ Do not include any explanatory text or markdown. Output must start with [ and end with ].
⚠️ Ensure all string values are wrapped in double quotes. Escape any internal quotes.

Known links to choose from: {json.dumps(extracted_links)}

DOCUMENT CONTENT:
\"\"\"
{extracted_text}
\"\"\"
"""

        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You extract structured regulatory updates from documents."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=4096
            )
            raw = response.choices[0].message.content.strip()
            cleaned = re.sub(r"^```json|```$", "", raw.strip(), flags=re.MULTILINE).strip()
            parsed = json.loads(cleaned)

            # Patch missing values and fuzzy match if link is blank
            for item in parsed:
                if "additional_context" not in item:
                    item["additional_context"] = ""
                if "link" not in item or not item["link"].strip():
                    topic = item.get("topic", "").lower()
                    match = get_close_matches(topic, extracted_links, n=1, cutoff=0.3)
                    item["link"] = match[0] if match else ""

            df = pd.DataFrame(parsed, columns=["date", "topic", "additional_context", "link", "regulator"])

        except Exception as e:
            print(f"⚠️ LLM extraction failed: {e}")
            df = pd.DataFrame(columns=["date", "topic", "additional_context", "link", "regulator"])

        domain = urlparse(url).netloc.replace('.', '_') if url else "unknown"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = OUTPUT_DIR / f"{domain}_llm_output_{timestamp}.csv"
        df.to_csv(output_path, index=False, encoding="utf-8-sig")

        print(f"✅ LLM-extracted data saved to: {output_path}")
        return {
            "url": url,
            "output_file": str(output_path)
        }

# Optional instance
llm_extractor_tool = LLMExtractorTool()
